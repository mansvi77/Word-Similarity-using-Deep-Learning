{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWLpoAPcwUfclIun43Da6e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mansvi77/Word-Similarity-using-Deep-Learning/blob/main/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IRYi-qyM1ikH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Reshape, Dot\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# ==========================================\n",
        "# ðŸ”§ THE FIX: Custom Skip-Gram Generator\n",
        "# ==========================================\n",
        "# The built-in Keras function is broken in new Python versions.\n",
        "# We write our own simple version here to replace it.\n",
        "def generate_skipgrams(sequence, vocabulary_size, window_size=2):\n",
        "    pairs = []\n",
        "    labels = []\n",
        "\n",
        "    # Loop through every word in the sentence (this is our 'target')\n",
        "    for i, target_word in enumerate(sequence):\n",
        "        # Determine the range of context words (e.g., 2 words back, 2 words forward)\n",
        "        start = max(0, i - window_size)\n",
        "        end = min(len(sequence), i + window_size + 1)\n",
        "\n",
        "        for j in range(start, end):\n",
        "            if i == j: continue # Skip the target word itself\n",
        "\n",
        "            context_word = sequence[j]\n",
        "\n",
        "            # 1. POSITIVE PAIR (The words actually appeared together)\n",
        "            pairs.append([target_word, context_word])\n",
        "            labels.append(1) # Label 1 means \"Yes, these are neighbors\"\n",
        "\n",
        "            # 2. NEGATIVE PAIR (Random Sampling)\n",
        "            # We pick a random word from the dictionary to teach the model\n",
        "            # what is NOT a neighbor.\n",
        "            random_word = int(random.randrange(1, vocabulary_size))\n",
        "\n",
        "            pairs.append([target_word, random_word])\n",
        "            labels.append(0) # Label 0 means \"No, this is random noise\"\n",
        "\n",
        "    return np.array(pairs), np.array(labels)\n",
        "\n",
        "# ==========================================\n",
        "# STEP 1: The Data (Our Tiny Library)\n",
        "# ==========================================\n",
        "corpus = [\n",
        "    \"the king is a royal man\",\n",
        "    \"the queen is a royal woman\",\n",
        "    \"royal men are kings\",\n",
        "    \"royal women are queens\",\n",
        "    \"the man is strong\",\n",
        "    \"the woman is strong\"\n",
        "]\n",
        "\n",
        "# ==========================================\n",
        "# STEP 2: Preprocessing (Text -> Numbers)\n",
        "# ==========================================\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "word2id = tokenizer.word_index\n",
        "id2word = {v:k for k,v in word2id.items()}\n",
        "\n",
        "vocab_size = len(word2id) + 1\n",
        "embed_size = 10\n",
        "\n",
        "print(f\"Total unique words: {vocab_size}\")\n",
        "print(f\"Word Map: {word2id}\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 3: Generate Training Pairs (Using our FIX)\n",
        "# ==========================================\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "\n",
        "targets = []\n",
        "contexts = []\n",
        "labels = []\n",
        "\n",
        "window_size = 2\n",
        "\n",
        "for seq in sequences:\n",
        "    # We use our custom function 'generate_skipgrams' instead of the broken Keras one\n",
        "    pairs, labs = generate_skipgrams(seq, vocabulary_size=vocab_size, window_size=window_size)\n",
        "\n",
        "    for p, l in zip(pairs, labs):\n",
        "        targets.append(p[0])\n",
        "        contexts.append(p[1])\n",
        "        labels.append(l)\n",
        "\n",
        "targets = np.array(targets)\n",
        "contexts = np.array(contexts)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(f\"\\nCreated {len(targets)} training pairs.\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 4: Build the Neural Network\n",
        "# ==========================================\n",
        "input_target = Input((1,), name='target_word_input')\n",
        "input_context = Input((1,), name='context_word_input')\n",
        "\n",
        "embedding_layer = Embedding(vocab_size, embed_size, input_length=1, name='word_embedding')\n",
        "\n",
        "target_vector = embedding_layer(input_target)\n",
        "context_vector = embedding_layer(input_context)\n",
        "\n",
        "# Dot Product checks similarity\n",
        "dot_product = Dot(axes=2, name='similarity_check')([target_vector, context_vector])\n",
        "dot_product = Reshape((1,), name='reshape')(dot_product)\n",
        "\n",
        "# Sigmoid output (0 to 1)\n",
        "output = Dense(1, activation='sigmoid', name='final_class')(dot_product)\n",
        "\n",
        "model = Model(inputs=[input_target, input_context], outputs=output)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# ==========================================\n",
        "# STEP 5: Train\n",
        "# ==========================================\n",
        "print(\"\\nTraining Model...\")\n",
        "model.fit(x=[targets, contexts], y=labels, batch_size=1, epochs=100, verbose=0)\n",
        "print(\"Training Complete.\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 6: Test Results\n",
        "# ==========================================\n",
        "learned_weights = model.get_layer('word_embedding').get_weights()[0]\n",
        "\n",
        "def get_similarity(word1, word2):\n",
        "    try:\n",
        "        id1 = word2id[word1]\n",
        "        id2 = word2id[word2]\n",
        "        vec1 = learned_weights[id1]\n",
        "        vec2 = learned_weights[id2]\n",
        "        dot = np.dot(vec1, vec2)\n",
        "        norm = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
        "        return dot / norm\n",
        "    except KeyError:\n",
        "        return 0\n",
        "\n",
        "print(\"\\n--- TEST RESULTS ---\")\n",
        "print(f\"Similarity 'king' vs 'man':    {get_similarity('king', 'man'):.4f}\")\n",
        "print(f\"Similarity 'king' vs 'queen':  {get_similarity('king', 'queen'):.4f}\")\n",
        "print(f\"Similarity 'king' vs 'strong': {get_similarity('king', 'strong'):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkYe4uIn3tOe",
        "outputId": "286e586c-469c-4b81-c760-f4fb7b5e72b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique words: 15\n",
            "Word Map: {'the': 1, 'is': 2, 'royal': 3, 'a': 4, 'man': 5, 'woman': 6, 'are': 7, 'strong': 8, 'king': 9, 'queen': 10, 'men': 11, 'kings': 12, 'women': 13, 'queens': 14}\n",
            "\n",
            "Created 152 training pairs.\n",
            "\n",
            "Training Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Complete.\n",
            "\n",
            "--- TEST RESULTS ---\n",
            "Similarity 'king' vs 'man':    0.5066\n",
            "Similarity 'king' vs 'queen':  0.6571\n",
            "Similarity 'king' vs 'strong': -0.1238\n"
          ]
        }
      ]
    }
  ]
}